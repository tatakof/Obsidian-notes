One of the challenges that arise in reinforcement learning, and not in other kinds
of learning, is the trade-o↵ between exploration and exploitation. To obtain a lot of
reward, a reinforcement learning agent must prefer actions that it has tried in the past
and found to be e↵ective in producing reward. But to discover such actions, it has to
try actions that it has not selected before. The agent has to exploit what it has already
experienced in order to obtain reward, but it also has to explore in order to make better
action selections in the future. The dilemma is that neither exploration nor exploitation
can be pursued exclusively without failing at the task. The agent must try a variety of
actions and progressively favor those that appear to be best. On a stochastic task, each
action must be tried many times to gain a reliable estimate of its expected reward. The
exploration–exploitation dilemma has been intensively studied by mathematicians for
many decades, yet remains unresolved. For now, we simply note that the entire issue of
balancing exploration and exploitation does not even arise in supervised and unsupervised
learning, at least in the purest forms of these paradigms.

