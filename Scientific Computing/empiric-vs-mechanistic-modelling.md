**FROM:** “MODEL: MECHANISTIC vs EMPIRICAL Ajit K. Thakur” (1991)

  
  

From a systems analytic standpoint, we need some terms to be defined before  
we can discuss the concept of a model. These definitions are elegantly expressed in  
Rescigno and Beck [1987]. Accordingly:  
  
  

(1) A system under study is the primary system.  
(2) Any aspect of this primary system that an investigator uses to study the system is  
a secondary system. The data or a graph representing the primary system will  
then be secondary system.  
(3) A model is a secondary system used to verify any hypotheses on the primary  
system.

  
  

  
  

Obviously, if one knew everything about the primary system, one would not  
need a model. Unfortunately, many of the real systems one wishes to study may not  
be wholly accessible. As a result, one has to use models to explain or verify certain  
aspects of such systems. Legal, social, religious, and ethical practices of the civilized  
world do not always allow us to pry into living subjects (some times not even dead  
subjects) to understand all intricate biological functions. So there must be needs for models.

  
  

For the purpose of our discussion, we will design two types of models:  
systems analytic or mechanistic and empirical or often designated as statistical  
(although the present author has some problem in using 'statistical' as a synonym for  
'empirical'). A mechanistic model, as the name implies, should have as many  
features of the primary system built into it as observations or data will allow. Such a  
model should be consistent with the observed behavior of the system - retrodiction  
- [Rescigno and Beck, 1987]; it should further be predictive of the system's future  
behavior or behavior under perturbation - prediction - [Rescigno and Beck,  
1987]. One must have some knowledge of the primary system in terms of structural  
connectivity and functional mechanisms. Some prefer to call this type of models  
realistic, intrinsic, and various other names. Many great discoveries in biology,  
medicine, and other branches of science have been made using such models. In this  
context one must remember that such models do not necessarily have to have an  
explicit mathematical expressions; they could be just conceptualizations.

  
  

On the other hand, when the system under study is complex and hardly  
anything is known about its structural connectivity and functional mechanisms, yet  
one has to produce hypotheses about it based on some external characteristics such  
as a dose-response (secondary system), one often relies on. mathematical functional  
forms for such a system. These mathematical functions are empirical models. They  
may incorporate some mechanistic assumptions so that they may look realistic.  
Numerically, these models are generally easier to handle as opposed to many  
mechanistic models. Most normal theory based statistical hypothesis testing and  
confidence interval procedures are based on such models. One should not get the  
wrong impression that mechanistic models are not useful for such statistical  
techniques; they may be more difficult to handle numerically from estimation  
standpoints. Some people would call empirical models extrinsic because they are  
based purely on the external behavior of the system. Some call them statistical  
models. As mentioned earlier, it is unfair to assume that statisticians always like to  
use empirical models for their purposes. The reasons why there are abundance of  
this type of models in literature are obvious. Our knowledge about the primary  
system may be inadequate-to-none to allow us the formulation of a mechanistic  
model or one may not be interested in understanding the inherent structure of the  
system. In the present author's mind, the phrase statistical model includes both types  
of models. One must remember that an empirical model may be 'retroactive'  
(explaining what happened from a secondary system) and even locally 'predictive'  
(Le. interpolation may be performed within the range of observations), but it is, in  
general, not globally 'predictive' (indicating outcome of future experiments). In fact,  
empirical models should never be used with any authority for extrapolative purposes.

A slight variation of empirical modeling is  
defined by Ashby [1958]. In this form, one takes the system and examines its  
individual components. One makes hypotheses on these individual components with  
mOdels and finally one tries to draw a global conclusion about the system. According to Ashby, the systems theoretic approaches by Bertalanffy [1950] fall in this category of empirical models. Most empirical models are generalized exponential or polynomial functions. The better ones of these also are mathematically well behaved.

  
  

[…]

[…]

[…]

We have seen several examples of mechanistic and empirical models in the  
foregoing discussion. As we saw, there are many instances where empirical models,  
because oftheir numerical simplicity, may be preferable or even necessary for certain  
types of analysis. Modeling is a technique used to understand and explain both the  
"knowns" and the "unknowns" of a system. It is especially important in biological  
sciences where often one is unable to probe into a living system at ones own will.  
Even where one may be able to physically isolate part of a system to study, its  
behavior or characteristics may not be exact or even appropriate when the part is  
intact and undisturbed.

In many ways, R.A. Fisher [1925] said it all:  
"A hypothesis is conceived and defined with all necessary exactitude; its logical  
consequences are ascertained by a deductive argument; these consequences are  
compared with the available observations; if these are. completely in accord with the  
deductions, the hypothesis is justified at least until fresh and more stringent  
observations are available."  
Unfortunately the zeal behind modeling is so overwhelming that some  
investigators refuse to heed the above lesson. They forget, irrespective of whether  
the model is mechanistic or empirical, that it is only as good as available information  
content from a system. When data are gathered under different experimental  
conditions, they should be willing to either modify or change their model if there is  
need for that Examples of such zealous endeavors are abun4ant in literature. We will  
have some brief discussions of them in several later chapters under carcinogenic risk  
assessment.

  
  

  
  

  
  

**FROM:** [https://www.cremeglobal.com/explaining-empirical-and-mechanistic-models/#:~:text=The%20first%20is%20a%20mechanistic,second%20is%20an%20empirical%20model.&text=An%20empirical%20model%2C%20sometimes%20called,that%20outcome%20in%20the%20future](https://www.cremeglobal.com/explaining-empirical-and-mechanistic-models/#:~:text=The%20first%20is%20a%20mechanistic,second%20is%20an%20empirical%20model.&text=An%20empirical%20model%2C%20sometimes%20called,that%20outcome%20in%20the%20future).

  
  

An empirical model, sometimes called a statistical model, relies on observation rather than theory. The idea is that if you observe some particular outcome following some particular circumstance then you can reliably predict that outcome in the future. 

  
  

These are all correlations and if you know anything about correlations it’s that (all together now!) correlation does not imply causation. But they don’t have to imply causation to be useful for prediction, they just have to be good signals and not mere background noise.

  
  

In fact, this is a common aim of much contemporary medical research where biomarkers are sought to predict disease or other physiological conditions. With the development of ‘omics technologies (genomics, transcriptomics, proteomics, metabolomics, etc, etc), high-throughput screens make it feasible to turn up biomarkers that are correlated with the condition of interest. It’s possible, with these technologies, to screen tens or hundreds of thousands of candidate markers looking for any that correlate with the disease or physiological condition of interest. With all these different biomarkers in play, inevitably some of the correlations found will be spurious, arising mostly through chance and with poor reproducibility, and this is what is meant by signal and noise. But even if all of the discovered biomarkers are noisy signals individually, it can none-the-less be, when taken as a group, that they provide reliable predictive power.

This is what happens under the hood in models which use machine learning. The algorithm sifts and re-sifts all the available data, comparing the combined signals each time with the outcome of interest and eventually returning the best set of predictive signals to the user to validate and test on a new independent set of data.

So correlation and, by extension, empirical models are not to be lightly dismissed. Often mechanistic information is not available, or requires lengthy, involved computation. Sometimes you don’t want to have to generate mechanistic information, because it will require conducting experiments and that can be costly, or time consuming. If your only concern is the reliability of the prediction, then a causative explanation of good signals is nice to have, but not necessary.

  
  

Mechanistic models, of course, have several advantages. Only a few input data points are required for a given prediction (amount of propellant, elevation and direction of aim in our artillery example above), whereas the number of observations needed for empirical models tends to grow exponentially with the number of variables included. Extrapolation is possible with mechanistic models. We can make good predictions outside the range of previously used input values. This is not the case with empirical models. If our shopper model above was developed for an electronics store, it won’t be useful for a sports store. Or if my dog is good at predicting when I will take him for a walk, he might be less skilled at the same prediction for my daughter (probably due to insufficient data…).

In truth, as referenced before, almost all models are a combination of mechanistic and empirical thinking. Mechanistic models must allow for some element of empiricism. If this weren’t so, then they would be capable of making predictions that were perfect to the Nth decimal place. The reverse is equally true. Empirical models include mechanistic elements, even if only in the selection of candidate predictor variables to investigate. The choice of one approach over the other is a false dichotomy and the utility of the model matters far more than the underlying approach.












Most of these approaches are based on the empirical  
detection of relationships and the construction of relative  
models that in brief capture all information about the  
response variable in relation to temperature. It should be  
noted that the presented temperature relationships can be  
judged as deterministic or empirical, by the sense that they  
consist of descriptions in which processes are not known,  
but where relations are established

